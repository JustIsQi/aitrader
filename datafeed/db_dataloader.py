from datetime import datetime

import pandas as pd
from loguru import logger

from config import DATA_DIR


class DbDataLoader:
    """æ•°æ®åº“æ‰¹é‡æŸ¥è¯¢æ•°æ®åŠ è½½å™¨ï¼ˆæ”¯æŒPostgreSQLï¼‰"""

    def __init__(self, auto_download=True):
        """
        Args:
            auto_download: æ˜¯å¦è‡ªåŠ¨ä¸‹è½½ç¼ºå¤±çš„æ•°æ®
        """
        self.auto_download = auto_download
        from database.pg_manager import get_db
        self.db = get_db()
        logger.info('DbDataLoader: ä½¿ç”¨ PostgreSQL ä½œä¸ºæ•°æ®æº')

    def _download_to_postgres(self, symbol):
        """ä¸‹è½½æ•°æ®å¹¶ç›´æ¥å†™å…¥ PostgreSQL"""
        try:
            from scripts.get_data import is_etf, fetch_stock_history_with_proxy, fetch_etf_history, fetch_stock_history
            from datetime import timedelta
            import time

            download_start = time.time()
            logger.info(f'ğŸ”„ [æ•°æ®ä¸‹è½½] {symbol} - å¼€å§‹å‡†å¤‡ä¸‹è½½...')

            # ä»æ•°æ®åº“è·å–æœ€æ–°æ—¥æœŸ
            if is_etf(symbol):
                last_db_date = self.db.get_latest_date(symbol)
            else:
                last_db_date = self.db.get_stock_latest_date(symbol)

            if last_db_date:
                next_day = last_db_date + timedelta(days=1)
                start_date = next_day.strftime('%Y%m%d')
                logger.info(f'ğŸ“… [æ•°æ®ä¸‹è½½] {symbol} - å¢é‡ä¸‹è½½æ¨¡å¼: ä» {start_date} å¼€å§‹')
            else:
                start_date = None
                logger.info(f'ğŸ“… [æ•°æ®ä¸‹è½½] {symbol} - å…¨é‡ä¸‹è½½æ¨¡å¼: è·å–æ‰€æœ‰å†å²æ•°æ®')

            # åˆ¤æ–­æ˜¯ ETF è¿˜æ˜¯è‚¡ç¥¨
            if is_etf(symbol):
                code = symbol.split('.')[0]
                logger.info(f'ğŸ“Š [æ•°æ®ä¸‹è½½] {symbol} - è¯†åˆ«ä¸ºETFï¼Œå¼€å§‹é€šè¿‡ä»£ç†è·å–æ•°æ® (å¯èƒ½éœ€è¦30-60ç§’)...')
                fetch_start = time.time()
                df = fetch_stock_history_with_proxy(code, func=fetch_etf_history,
                                                     start_date=start_date, end_date=None)
                fetch_elapsed = time.time() - fetch_start
                logger.info(f'â±ï¸ [æ•°æ®ä¸‹è½½] {symbol} - ç½‘ç»œè¯·æ±‚å®Œæˆï¼Œè€—æ—¶ {fetch_elapsed:.1f}ç§’')
            else:
                code = symbol.split('.')[0]
                logger.info(f'ğŸ“Š [æ•°æ®ä¸‹è½½] {symbol} - è¯†åˆ«ä¸ºè‚¡ç¥¨ï¼Œå¼€å§‹é€šè¿‡ä»£ç†è·å–æ•°æ® (å¯èƒ½éœ€è¦30-60ç§’)...')
                fetch_start = time.time()
                df = fetch_stock_history_with_proxy(code, func=fetch_stock_history,
                                                     start_date=start_date, end_date=None)
                fetch_elapsed = time.time() - fetch_start
                logger.info(f'â±ï¸ [æ•°æ®ä¸‹è½½] {symbol} - ç½‘ç»œè¯·æ±‚å®Œæˆï¼Œè€—æ—¶ {fetch_elapsed:.1f}ç§’')

            if df is None or df.empty:
                logger.error(f'âŒ [æ•°æ®ä¸‹è½½] {symbol} - è·å–æ•°æ®ä¸ºç©ºæˆ–å¤±è´¥')
                return None

            logger.info(f'âœ“ [æ•°æ®ä¸‹è½½] {symbol} - æˆåŠŸè·å– {len(df)} æ¡è®°å½•')

            # è½¬æ¢åˆ—åä¸ºè‹±æ–‡
            if 'æ—¥æœŸ' in df.columns:
                df.rename(columns={'æ—¥æœŸ': 'date', 'è‚¡ç¥¨ä»£ç ': 'symbol',
                                   'å¼€ç›˜': 'open', 'æ”¶ç›˜': 'close',
                                   'æœ€é«˜': 'high', 'æœ€ä½': 'low',
                                   'æˆäº¤é‡': 'volume', 'æˆäº¤é¢': 'amount',
                                   'æ¶¨è·Œå¹…': 'change_pct', 'æ¶¨è·Œé¢': 'change_amount',
                                   'æŒ¯å¹…': 'amplitude', 'æ¢æ‰‹ç‡': 'turnover_rate'}, inplace=True)
                logger.debug(f'ğŸ“ [æ•°æ®ä¸‹è½½] {symbol} - åˆ—åå·²è½¬æ¢ä¸ºè‹±æ–‡æ ¼å¼')

            # æ·»åŠ  symbol åˆ—
            df['symbol'] = symbol

            # æ˜¾ç¤ºæ•°æ®èŒƒå›´
            if 'date' in df.columns:
                date_range = f"{df['date'].min()} ~ {df['date'].max()}"
                logger.info(f'ğŸ“… [æ•°æ®ä¸‹è½½] {symbol} - æ•°æ®èŒƒå›´: {date_range}')

            # ç›´æ¥å†™å…¥ PostgreSQL
            logger.info(f'ğŸ’¾ [æ•°æ®åº“] {symbol} - å¼€å§‹å†™å…¥æ•°æ®åº“...')
            write_start = time.time()
            if is_etf(symbol):
                success = self.db.append_etf_history(df, symbol)
                table_name = 'etf_history'
            else:
                success = self.db.append_stock_history(df, symbol)
                table_name = 'stock_history'
            write_elapsed = time.time() - write_start

            if success:
                total_elapsed = time.time() - download_start
                logger.success(f'âœ… [æ•°æ®ä¸‹è½½] {symbol} - å®Œæˆ! å†™å…¥è€—æ—¶ {write_elapsed:.2f}ç§’ | æ€»è€—æ—¶ {total_elapsed:.2f}ç§’ | è®°å½•æ•°: {len(df)}')
            else:
                logger.error(f'âŒ [æ•°æ®åº“] {symbol} - æ•°æ®å†™å…¥å¤±è´¥')

            return df if success else None

        except Exception as e:
            total_elapsed = time.time() - download_start if 'download_start' in locals() else 0
            logger.error(f'âŒ [æ•°æ®ä¸‹è½½] {symbol} - ä¸‹è½½å¤±è´¥ (è€—æ—¶ {total_elapsed:.2f}ç§’): {e}')
            import traceback
            logger.debug(f'ğŸ” [é”™è¯¯è¯¦æƒ…]\n{traceback.format_exc()}')
            return None

    def _read_postgres(self, symbol, start_date, end_date):
        """ä» PostgreSQL è¯»å–æ•°æ®"""
        try:
            # è½¬æ¢æ—¥æœŸæ ¼å¼
            start_date_fmt = start_date[:4] + '-' + start_date[4:6] + '-' + start_date[6:]
            end_date_fmt = end_date[:4] + '-' + end_date[4:6] + '-' + end_date[6:]

            # åˆ¤æ–­æ˜¯ ETF è¿˜æ˜¯è‚¡ç¥¨
            from scripts.get_data import is_etf
            if is_etf(symbol):
                df = self.db.get_etf_history(symbol, start_date=start_date_fmt, end_date=end_date_fmt)
            else:
                df = self.db.get_stock_history(symbol, start_date=start_date_fmt, end_date=end_date_fmt)

            if df.empty:
                if self.auto_download:
                    logger.info(f'PostgreSQL ä¸­æ—  {symbol} æ•°æ®ï¼Œå¼€å§‹ä¸‹è½½...')
                    # å°è¯•ä¸‹è½½æ•°æ®åˆ° PostgreSQL
                    df = self._download_to_postgres(symbol)
                    if df is not None:
                        # ç»Ÿä¸€æ—¥æœŸæ ¼å¼ä¸º YYYYMMDDï¼ˆç§»é™¤æ¨ªæ ï¼‰
                        df['date'] = pd.to_datetime(df['date']).dt.strftime('%Y%m%d')
                        df['symbol'] = symbol
                        df.dropna(inplace=True)
                        return df
                else:
                    logger.warning(f'PostgreSQL ä¸­æ—  {symbol} æ•°æ®ï¼ˆauto_download=Falseï¼Œè·³è¿‡ä¸‹è½½ï¼‰')
                return None

            # è½¬æ¢æ—¥æœŸæ ¼å¼ä¸º YYYYMMDD
            df['date'] = pd.to_datetime(df['date']).dt.strftime('%Y%m%d')

            # æ·»åŠ  symbol åˆ—
            df['symbol'] = symbol

            df.dropna(inplace=True)
            return df

        except Exception as e:
            logger.error(f'ä» PostgreSQL è¯»å– {symbol} å¤±è´¥: {e}')
            return None

    def read_dfs(self, symbols: list[str], start_date='20100101', end_date=datetime.now().strftime('%Y%m%d')):
        """è¯»å–å¤šä¸ªæ ‡çš„çš„æ•°æ®ï¼ˆæ‰¹é‡æŸ¥è¯¢ä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        # â­ ADD: Performance monitoring
        import time
        from concurrent.futures import ThreadPoolExecutor

        load_start = time.time()

        # æ£€æŸ¥ symbols æ˜¯å¦ä¸ºç©º
        if not symbols:
            raise ValueError("æ²¡æœ‰æä¾›ä»»ä½•æ ‡çš„ä»£ç ã€‚è¯·ç¡®ä¿ç­–ç•¥ç”Ÿæˆäº†æœ‰æ•ˆçš„è‚¡ç¥¨/ETFåˆ—è¡¨ã€‚")

        from scripts.get_data import is_etf

        # è½¬æ¢æ—¥æœŸæ ¼å¼
        start_date_fmt = start_date[:4] + '-' + start_date[4:6] + '-' + start_date[6:]
        end_date_fmt = end_date[:4] + '-' + end_date[4:6] + '-' + end_date[6:]

        # â­ ADD: Log request details for monitoring
        logger.info(f'DbDataLoader: å¼€å§‹åŠ è½½ {len(symbols)} ä¸ªæ ‡çš„çš„æ•°æ® ({start_date_fmt} ~ {end_date_fmt})')

        # åˆ†ç¦»ETFå’Œè‚¡ç¥¨
        etf_symbols = [s for s in symbols if is_etf(s)]
        stock_symbols = [s for s in symbols if not is_etf(s)]

        dfs = {}

        # â­ OPTIMIZATION 1: Adaptive batch sizes based on symbol count
        # ETF batches: Keep 100 (ETFs are fewer, queries are faster)
        ETF_BATCH_SIZE = 100

        # Stock batches: Scale based on total stocks
        # - < 500 stocks: batch size = stock count (single query)
        # - 500-2000 stocks: batch size = 500
        # - > 2000 stocks: batch size = 1000
        if len(stock_symbols) < 500:
            STOCK_BATCH_SIZE = len(stock_symbols)  # Single batch
        elif len(stock_symbols) < 2000:
            STOCK_BATCH_SIZE = 500
        else:
            STOCK_BATCH_SIZE = 1000

        logger.debug(f'æ‰¹é‡æŸ¥è¯¢é…ç½®: ETF_BATCH={ETF_BATCH_SIZE}, STOCK_BATCH={STOCK_BATCH_SIZE}')

        # â­ OPTIMIZATION 2: Define batch loading functions
        def load_etf_batch():
            """Load all ETF batches"""
            results = {}
            if not etf_symbols:
                return results

            batch_start = time.time()
            for i in range(0, len(etf_symbols), ETF_BATCH_SIZE):
                batch = etf_symbols[i:i+ETF_BATCH_SIZE]
                try:
                    logger.debug(f'æ‰¹é‡æŸ¥è¯¢ETF: ç¬¬ {i//ETF_BATCH_SIZE + 1} æ‰¹ï¼Œå…± {len(batch)} åªETF')
                    query_start = time.time()

                    # âœ… Date filtering happens in SQL (fast)
                    df_all = self.db.batch_get_etf_history(batch, start_date_fmt, end_date_fmt)

                    query_elapsed = time.time() - query_start
                    logger.debug(f'  æŸ¥è¯¢è€—æ—¶: {query_elapsed:.2f}ç§’, è¿”å› {len(df_all)} è¡Œ')

                    if not df_all.empty:
                        # âœ… OPTIMIZATION 3: Use groupby instead of loop for symbol filtering
                        for symbol, group in df_all.groupby('symbol'):
                            group = group.copy()
                            group['date'] = pd.to_datetime(group['date']).dt.strftime('%Y%m%d')
                            group.dropna(inplace=True)
                            results[symbol] = group
                    else:
                        logger.warning(f'æ‰¹é‡æŸ¥è¯¢ETFï¼ˆç¬¬ {i//ETF_BATCH_SIZE + 1} æ‰¹ï¼‰æœªè¿”å›æ•°æ®')

                except Exception as e:
                    logger.error(f'æ‰¹é‡æŸ¥è¯¢ETFå¤±è´¥ï¼ˆç¬¬ {i//ETF_BATCH_SIZE + 1} æ‰¹ï¼‰: {e}ï¼Œå›é€€åˆ°å•ä¸ªæŸ¥è¯¢')
                    # Fallback to individual queries
                    for s in batch:
                        df = self._read_postgres(s, start_date, end_date)
                        if df is not None:
                            results[s] = df

            batch_elapsed = time.time() - batch_start
            logger.info(f'âœ“ ETFæ•°æ®åŠ è½½å®Œæˆ: {len(results)} ä¸ªæ ‡çš„, è€—æ—¶ {batch_elapsed:.2f}ç§’')
            return results

        def load_stock_batch():
            """Load all stock batches"""
            results = {}
            if not stock_symbols:
                return results

            batch_start = time.time()
            for i in range(0, len(stock_symbols), STOCK_BATCH_SIZE):
                batch = stock_symbols[i:i+STOCK_BATCH_SIZE]
                try:
                    logger.debug(f'æ‰¹é‡æŸ¥è¯¢è‚¡ç¥¨: ç¬¬ {i//STOCK_BATCH_SIZE + 1} æ‰¹ï¼Œå…± {len(batch)} åªè‚¡ç¥¨')
                    query_start = time.time()

                    # âœ… Date filtering happens in SQL (fast)
                    df_all = self.db.batch_get_stock_history(batch, start_date_fmt, end_date_fmt)

                    query_elapsed = time.time() - query_start
                    logger.debug(f'  æŸ¥è¯¢è€—æ—¶: {query_elapsed:.2f}ç§’, è¿”å› {len(df_all)} è¡Œ')

                    if not df_all.empty:
                        # âœ… OPTIMIZATION 3: Use groupby instead of loop for symbol filtering
                        for symbol, group in df_all.groupby('symbol'):
                            group = group.copy()
                            group['date'] = pd.to_datetime(group['date']).dt.strftime('%Y%m%d')
                            group.dropna(inplace=True)
                            results[symbol] = group
                    else:
                        logger.warning(f'æ‰¹é‡æŸ¥è¯¢è‚¡ç¥¨ï¼ˆç¬¬ {i//STOCK_BATCH_SIZE + 1} æ‰¹ï¼‰æœªè¿”å›æ•°æ®')

                except Exception as e:
                    logger.error(f'æ‰¹é‡æŸ¥è¯¢è‚¡ç¥¨å¤±è´¥ï¼ˆç¬¬ {i//STOCK_BATCH_SIZE + 1} æ‰¹ï¼‰: {e}ï¼Œå›é€€åˆ°å•ä¸ªæŸ¥è¯¢')
                    # Fallback to individual queries
                    for s in batch:
                        df = self._read_postgres(s, start_date, end_date)
                        if df is not None:
                            results[s] = df

            batch_elapsed = time.time() - batch_start
            logger.info(f'âœ“ è‚¡ç¥¨æ•°æ®åŠ è½½å®Œæˆ: {len(results)} ä¸ªæ ‡çš„, è€—æ—¶ {batch_elapsed:.2f}ç§’')
            return results

        # â­ OPTIMIZATION 4: Parallel processing of ETFs and stocks
        with ThreadPoolExecutor(max_workers=2) as executor:
            future_etf = executor.submit(load_etf_batch)
            future_stock = executor.submit(load_stock_batch)

            dfs.update(future_etf.result())
            dfs.update(future_stock.result())

        if not dfs:
            missing_symbols = [s for s in symbols if s not in dfs]
            raise ValueError(f"æ²¡æœ‰å¯ç”¨çš„æ•°æ®ã€‚ä»¥ä¸‹æ ‡çš„æ•°æ®ç¼ºå¤±: {missing_symbols}ã€‚å·²å°è¯•è‡ªåŠ¨ä¸‹è½½ä½†ä»å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ä»£ç†è®¾ç½®ã€‚")

        # â­ NEW: åˆå¹¶åŸºæœ¬é¢æ•°æ®ï¼ˆä»…è‚¡ç¥¨ï¼ŒETFä¸éœ€è¦ï¼‰
        if stock_symbols:
            dfs = self._merge_fundamental_data(dfs, stock_symbols)

        total_elapsed = time.time() - load_start
        logger.success(f'âœ“ æ•°æ®åŠ è½½å…¨éƒ¨å®Œæˆ: {len(dfs)} ä¸ªæ ‡çš„, æ€»è€—æ—¶ {total_elapsed:.2f}ç§’')

        return dfs

    def _merge_fundamental_data(self, dfs: dict, stock_symbols: list) -> dict:
        """
        åˆå¹¶åŸºæœ¬é¢æ•°æ®ï¼ˆPEã€PBï¼‰åˆ°è‚¡ç¥¨å†å²æ•°æ®ä¸­

        Args:
            dfs: è‚¡ç¥¨å†å²æ•°æ®å­—å…¸ {symbol: DataFrame}
            stock_symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨

        Returns:
            åˆå¹¶åçš„æ•°æ®å­—å…¸
        """
        try:
            # æ‰¹é‡è·å–åŸºæœ¬é¢æ•°æ®
            fundamental_df = self.db.batch_get_latest_fundamental(stock_symbols)

            if fundamental_df.empty:
                logger.warning('æœªè·å–åˆ°åŸºæœ¬é¢æ•°æ®ï¼ŒPE/PBç­‰å­—æ®µå°†ä¸ºç©º')
                return dfs

            logger.info(f'âœ“ å·²è·å– {len(fundamental_df)} åªè‚¡ç¥¨çš„åŸºæœ¬é¢æ•°æ® (PE/PB)')

            # å°†åŸºæœ¬é¢æ•°æ®è½¬ä¸ºå­—å…¸ {symbol: {pe: xxx, pb: xxx}}
            fund_dict = fundamental_df.set_index('symbol').to_dict('index')

            # ä¸ºæ¯åªè‚¡ç¥¨çš„DataFrameæ·»åŠ åŸºæœ¬é¢åˆ—
            for symbol, df in dfs.items():
                if symbol in fund_dict:
                    fund_data = fund_dict[symbol]
                    # æ·»åŠ PEå’ŒPBåˆ—ï¼ˆæ‰€æœ‰è¡Œä½¿ç”¨ç›¸åŒçš„æœ€æ–°å€¼ï¼‰
                    df['pe'] = fund_data.get('pe')
                    df['pb'] = fund_data.get('pb')
                else:
                    # æ²¡æœ‰åŸºæœ¬é¢æ•°æ®çš„è‚¡ç¥¨ï¼Œè®¾ä¸ºNaN
                    df['pe'] = None
                    df['pb'] = None

            return dfs

        except Exception as e:
            logger.error(f'åˆå¹¶åŸºæœ¬é¢æ•°æ®å¤±è´¥: {e}')
            return dfs


# å‘åå…¼å®¹ï¼šä¿ç•™ CsvDataLoader åˆ«å
CsvDataLoader = DbDataLoader


if __name__ == '__main__':
    df = DbDataLoader().read_dfs(symbols=['510300.SH', '159915.SZ'])
    print(df)
