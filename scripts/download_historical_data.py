"""
å†å²æ•°æ®ä¸‹è½½è„šæœ¬
ä¸€æ¬¡æ€§ä¸‹è½½è¿‡å»Nå¹´çš„ETFå’ŒAè‚¡å†å²æ•°æ®åˆ°PostgreSQLæ•°æ®åº“

åŠŸèƒ½:
    1. ä¸‹è½½ETFå†å²æ•°æ® -> å­˜å…¥ etf_history è¡¨
    2. ä¸‹è½½Aè‚¡å†å²æ•°æ® -> å­˜å…¥ stock_history è¡¨
    3. ä¸‹è½½åŸºæœ¬é¢æ•°æ®(ä»…PE/PB) -> å­˜å…¥ stock_fundamental_daily è¡¨
    4. è‡ªåŠ¨åˆå§‹åŒ–ä»£ç è¡¨ -> etf_codes å’Œ stock_codes

ç”¨æ³•:
    python scripts/download_historical_data.py                        # ä¸‹è½½é»˜è®¤5å¹´å†å²æ•°æ®(å…¨éƒ¨ç±»å‹)
    python scripts/download_historical_data.py --years 3             # ä¸‹è½½è¿‘3å¹´æ•°æ®
    python scripts/download_historical_data.py -t etf stock          # åªä¸‹è½½ETFå’Œè‚¡ç¥¨å†å²æ•°æ®
    python scripts/download_historical_data.py -t fundamental        # åªä¸‹è½½åŸºæœ¬é¢å¿«ç…§(ä»…PE/PB)
    python scripts/download_historical_data.py --force               # å¼ºåˆ¶é‡æ–°ä¸‹è½½(è¦†ç›–å·²æœ‰æ•°æ®)
    
æ³¨æ„:
    - åŸºæœ¬é¢æ•°æ®ä»…ä¸‹è½½PE(å¸‚ç›ˆç‡)å’ŒPB(å¸‚å‡€ç‡)ï¼Œå…¶ä»–å­—æ®µè®¾ä¸ºNULL
    - æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼Œé‡æ–°è¿è¡Œä¼šè‡ªåŠ¨è·³è¿‡å·²æœ‰æ•°æ®
"""     
import sys
import time
import argparse
from pathlib import Path
from datetime import datetime, timedelta, date
from typing import List, Optional, NamedTuple
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor, as_completed

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import pandas as pd
from loguru import logger
from tqdm import tqdm
from database.pg_manager import get_db
from datafeed.downloaders.etf_downloader import EtfDownloader
from datafeed.downloaders.stock_downloader import StockDownloader
from datafeed.downloaders.fundamental_downloader import FundamentalDownloader
from datafeed.downloaders.rate_limiter import RateLimiter
from scripts.init_codes import CodeInitializer


class DownloadDecision(NamedTuple):
    """ä¸‹è½½å†³ç­–ç»“æœ"""
    should_download: bool
    actual_start_date: Optional[str]
    reason: str


@dataclass
class DownloadStats:
    """ä¸‹è½½ç»Ÿè®¡ä¿¡æ¯"""
    start_time: datetime = field(default_factory=datetime.now)
    end_time: Optional[datetime] = None

    etf_stats: dict = field(default_factory=lambda: {
        'total': 0, 'downloaded': 0, 'skipped': 0,
        'failed': 0, 'records_added': 0, 'duration': 0
    })
    stock_stats: dict = field(default_factory=lambda: {
        'total': 0, 'downloaded': 0, 'skipped': 0,
        'failed': 0, 'records_added': 0, 'duration': 0
    })
    fundamental_stats: dict = field(default_factory=lambda: {
        'total': 0, 'success': 0, 'failed': 0, 'duration': 0
    })


class HistoricalDataDownloader:
    """å†å²æ•°æ®ä¸‹è½½å™¨ - è´Ÿè´£ä¸‹è½½è¿‡å»Nå¹´çš„æ•°æ®"""

    def __init__(self, max_workers: int = 5):
        self.db = get_db()
        self.etf_downloader = EtfDownloader()
        self.stock_downloader = StockDownloader()
        self.fundamental_downloader = FundamentalDownloader()
        self.stats = DownloadStats()
        self.failed_symbols = {'etf': [], 'stock': []}
        self.max_workers = max_workers  # å¹¶å‘çº¿ç¨‹æ•°
        self.rate_limiter = RateLimiter(max_requests_per_second=5)  # APIé™æµå™¨

    def check_and_init_codes(self, force_refresh: bool = False) -> bool:
        """
        æ£€æŸ¥å¹¶è‡ªåŠ¨åˆå§‹åŒ–ä»£ç è¡¨

        Args:
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ä»£ç è¡¨ï¼ˆæ¸…ç©ºå¹¶é‡æ–°ä¸‹è½½ï¼‰

        Returns:
            bool: æ˜¯å¦æ‰§è¡Œäº†åˆå§‹åŒ–
        """
        logger.info('[é˜¶æ®µ0] æ£€æŸ¥ä»£ç è¡¨çŠ¶æ€...')

        code_count = self.db.get_code_count()
        etf_empty = code_count.get('etf', 0) == 0
        stock_empty = code_count.get('stock', 0) == 0

        logger.info(f'  etf_codes:   {code_count.get("etf", 0)} æ¡')
        logger.info(f'  stock_codes: {code_count.get("stock", 0)} æ¡')

        if force_refresh or etf_empty or stock_empty:
            if force_refresh:
                logger.info('')
                logger.info('ğŸ”„ å¼ºåˆ¶åˆ·æ–°ä»£ç è¡¨...')
                logger.info('')
            else:
                logger.warning('')
                logger.warning('âš ï¸  ä»£ç è¡¨ä¸ºç©º,å¼€å§‹è‡ªåŠ¨åˆå§‹åŒ–...')
                logger.warning('')

            initializer = CodeInitializer()
            initializer.init_all_codes(force=force_refresh)

            logger.info('')
            logger.info('âœ“ ä»£ç è¡¨åˆå§‹åŒ–å®Œæˆ')
            logger.info('')
            return True

        logger.info('âœ“ ä»£ç è¡¨çŠ¶æ€æ­£å¸¸')
        logger.info('')
        return False

    def calculate_date_range(self, years: int) -> tuple[str, str]:
        """
        è®¡ç®—ä¸‹è½½æ—¥æœŸèŒƒå›´

        Args:
            years: å†å²å¹´æ•°

        Returns:
            (start_date, end_date) in YYYYMMDD format
        """
        end_date = datetime.now()
        start_date = end_date - timedelta(days=years*365)

        # å¯¹é½åˆ°1æœˆ1æ—¥ï¼Œä½¿æ•°æ®æ›´æ•´æ´
        start_date = start_date.replace(month=1, day=1)

        start_date_str = start_date.strftime('%Y%m%d')
        end_date_str = end_date.strftime('%Y%m%d')

        logger.info(f'ç›®æ ‡æ—¥æœŸèŒƒå›´: {start_date.strftime("%Y-%m-%d")} è‡³ {end_date.strftime("%Y-%m-%d")} ({years}å¹´)')

        return start_date_str, end_date_str

    def _should_download(self, symbol: str, latest_date: Optional[datetime],
                        target_start: str, force: bool) -> DownloadDecision:
        """
        åˆ¤æ–­æ˜¯å¦éœ€è¦ä¸‹è½½æ•°æ®

        Args:
            symbol: è‚¡ç¥¨/ETFä»£ç 
            latest_date: æ•°æ®åº“ä¸­æœ€æ–°çš„æ—¥æœŸ (å¯èƒ½æ˜¯ datetime.date æˆ– datetime.datetime)
            target_start: ç›®æ ‡èµ·å§‹æ—¥æœŸ (YYYYMMDD)
            force: æ˜¯å¦å¼ºåˆ¶é‡æ–°ä¸‹è½½

        Returns:
            DownloadDecision
        """
        target_start_dt = datetime.strptime(target_start, '%Y%m%d')

        if force:
            return DownloadDecision(
                should_download=True,
                actual_start_date=target_start,
                reason="å¼ºåˆ¶é‡æ–°ä¸‹è½½"
            )

        if latest_date is None:
            return DownloadDecision(
                should_download=True,
                actual_start_date=target_start,
                reason="é¦–æ¬¡ä¸‹è½½"
            )

        # ç¡®ä¿ latest_date æ˜¯ datetime.datetime ç±»å‹
        if isinstance(latest_date, datetime):
            latest_date_dt = latest_date
        else:
            # å¦‚æœæ˜¯ datetime.dateï¼Œè½¬æ¢ä¸º datetime.datetime
            latest_date_dt = datetime.combine(latest_date, datetime.min.time())

        if latest_date_dt < target_start_dt:
            return DownloadDecision(
                should_download=True,
                actual_start_date=target_start,
                reason=f"æ•°æ®ä¸è¶³ (æœ€æ–°: {latest_date_dt.strftime('%Y-%m-%d')})"
            )

        # æ£€æŸ¥æ˜¯å¦éœ€è¦å¢é‡æ›´æ–°
        today = datetime.now()
        if latest_date_dt < today:
            next_day = latest_date_dt + timedelta(days=1)
            return DownloadDecision(
                should_download=True,
                actual_start_date=next_day.strftime('%Y%m%d'),
                reason=f"å¢é‡æ›´æ–° (ä» {latest_date_dt.strftime('%Y-%m-%d')})"
            )

        return DownloadDecision(
            should_download=False,
            actual_start_date=None,
            reason="æ•°æ®å·²æ˜¯æœ€æ–°"
        )

    def _download_single_batch(self, symbols_batch: List[str], start_date: str, end_date: str,
                              is_etf_batch: bool = False) -> dict:
        """
        ä¸‹è½½ä¸€æ‰¹è‚¡ç¥¨/ETFæ•°æ®ï¼ˆå¸¦é™æµå’Œé”™è¯¯å¤„ç†ï¼‰

        Args:
            symbols_batch: ä¸€æ‰¹è‚¡ç¥¨/ETFä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            is_etf_batch: æ˜¯å¦ä¸ºETFæ‰¹æ¬¡ï¼ˆé»˜è®¤Falseï¼Œè¡¨ç¤ºè‚¡ç¥¨æ‰¹æ¬¡ï¼‰

        Returns:
            dict: æ‰¹æ¬¡ç»Ÿè®¡ä¿¡æ¯
        """
        batch_data = []
        failed_symbols = []

        for symbol in symbols_batch:
            try:
                # ä½¿ç”¨é™æµå™¨æ§åˆ¶APIè¯·æ±‚é€Ÿç‡
                self.rate_limiter.acquire()

                code = symbol.split('.')[0]

                # æ ¹æ®æ‰¹æ¬¡ç±»å‹é€‰æ‹©ä¸‹è½½å™¨
                if is_etf_batch:
                    df = self.etf_downloader.fetch_etf_history(code, start_date, end_date)
                else:
                    df = self.stock_downloader.fetch_stock_history(code, start_date, end_date)

                if df is not None and not df.empty:
                    # è½¬æ¢åˆ—å
                    df.rename(columns={
                        'æ—¥æœŸ': 'date', 'å¼€ç›˜': 'open', 'æ”¶ç›˜': 'close',
                        'æœ€é«˜': 'high', 'æœ€ä½': 'low', 'æˆäº¤é‡': 'volume',
                        'æˆäº¤é¢': 'amount', 'æŒ¯å¹…': 'amplitude', 'æ¶¨è·Œå¹…': 'change_pct',
                        'æ¶¨è·Œé¢': 'change_amount', 'æ¢æ‰‹ç‡': 'turnover_rate'
                    }, inplace=True)
                    df['symbol'] = symbol
                    batch_data.append(df)
                else:
                    failed_symbols.append(symbol)

            except Exception as e:
                logger.debug(f'{symbol} ä¸‹è½½å¤±è´¥: {e}')
                failed_symbols.append(symbol)

        # æ‰¹é‡æ’å…¥æ•°æ®åº“
        records_added = 0
        if batch_data:
            try:
                combined_df = pd.concat(batch_data, ignore_index=True)

                # æ ¹æ®æ‰¹æ¬¡ç±»å‹é€‰æ‹©æ‰¹é‡æ’å…¥æ–¹æ³•
                if is_etf_batch:
                    records_added = self.db.batch_append_etf_history(combined_df)
                else:
                    records_added = self.db.batch_append_stock_history(combined_df)

            except Exception as e:
                logger.error(f'æ‰¹é‡æ’å…¥å¤±è´¥: {e}')
                failed_symbols.extend([s for df in batch_data for s in df['symbol'].unique()])

        return {
            'total': len(symbols_batch),
            'success': len(batch_data),
            'failed': len(failed_symbols),
            'failed_symbols': failed_symbols,
            'records_added': records_added
        }

    def download_etf_history_optimized(self, start_date: str, end_date: str,
                                       force: bool = False) -> dict:
        """
        ä¼˜åŒ–çš„ETFå†å²æ•°æ®ä¸‹è½½ï¼ˆæ™ºèƒ½æ‰¹é‡æ¨¡å¼ï¼‰

        ä¸‰é˜¶æ®µç­–ç•¥:
        1. æ‰¹é‡æ£€æŸ¥å®Œæ•´æ€§ - å•æ¬¡æŸ¥è¯¢æ‰€æœ‰ETF
        2. åˆ†æ‰¹å¤„ç† - å°†éœ€è¦ä¸‹è½½çš„ETFåˆ†æˆæ‰¹æ¬¡
        3. å¹¶å‘ä¸‹è½½ + æ‰¹é‡æ’å…¥ - ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†æ‰¹æ¬¡

        Args:
            start_date: å¼€å§‹æ—¥æœŸ (YYYYMMDD)
            end_date: ç»“æŸæ—¥æœŸ (YYYYMMDD)
            force: æ˜¯å¦å¼ºåˆ¶é‡æ–°ä¸‹è½½

        Returns:
            dict: ç»Ÿè®¡ä¿¡æ¯
        """
        logger.info('='*60)
        logger.info('[é˜¶æ®µ1] ä¸‹è½½ETFå†å²æ•°æ® (æ™ºèƒ½æ‰¹é‡æ¨¡å¼)')
        logger.info('='*60)
        logger.info('')

        start = time.time()
        symbols = self.db.get_etf_codes()

        if not symbols:
            logger.warning('æ²¡æœ‰æ‰¾åˆ° ETF ä»£ç ,è·³è¿‡ ETF ä¸‹è½½')
            return {'total': 0, 'downloaded': 0, 'skipped': 0, 'failed': 0, 'records_added': 0, 'duration': 0}

        # Phase 1: æ‰¹é‡æ£€æŸ¥å®Œæ•´æ€§
        logger.info(f'[Phase 1] æ£€æŸ¥ {len(symbols)} ä¸ªETFçš„æ•°æ®å®Œæ•´æ€§...')
        completeness = self.db.get_etf_completeness_info(symbols, start_date)

        needs_download = [s for s, info in completeness.items()
                         if info['needs_download'] or force]
        already_complete = len(symbols) - len(needs_download)

        logger.info(f'  å·²å®Œæ•´: {already_complete} ä¸ª (è·³è¿‡)')
        logger.info(f'  éœ€ä¸‹è½½: {len(needs_download)} ä¸ª')
        logger.info('')

        if not needs_download:
            return {
                'total': len(symbols),
                'downloaded': 0,
                'skipped': len(symbols),
                'failed': 0,
                'records_added': 0,
                'duration': 0
            }

        # Phase 2: åˆ†æ‰¹å¤„ç†
        BATCH_SIZE = 50
        symbol_batches = [needs_download[i:i+BATCH_SIZE]
                         for i in range(0, len(needs_download), BATCH_SIZE)]

        logger.info(f'[Phase 2] å¤„ç† {len(symbol_batches)} ä¸ªæ‰¹æ¬¡ (æ¯æ‰¹{BATCH_SIZE}ä¸ª)...')
        logger.info(f'å¹¶å‘æ•°: {self.max_workers} ä¸ªçº¿ç¨‹')
        logger.info('')

        stats = {
            'total': len(symbols),
            'downloaded': 0,
            'skipped': already_complete,
            'failed': 0,
            'records_added': 0
        }

        # Phase 3: å¹¶å‘ä¸‹è½½æ‰¹æ¬¡
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {
                executor.submit(self._download_single_batch, batch, start_date, end_date, is_etf_batch=True): batch
                for batch in symbol_batches
            }

            with tqdm(total=len(symbol_batches), desc="ETFæ‰¹é‡ä¸‹è½½", unit="æ‰¹") as pbar:
                for future in as_completed(futures):
                    try:
                        batch_stats = future.result()
                        stats['downloaded'] += batch_stats['success']
                        stats['failed'] += batch_stats['failed']
                        stats['records_added'] += batch_stats['records_added']
                        self.failed_symbols['etf'].extend(batch_stats['failed_symbols'])

                        pbar.set_postfix({
                            'ä¸‹è½½': stats['downloaded'],
                            'å¤±è´¥': stats['failed']
                        })

                    except Exception as e:
                        logger.error(f'æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}')
                        stats['failed'] += len(futures[future])

                    pbar.update(1)

        duration = time.time() - start
        stats['duration'] = duration

        logger.info('')
        logger.info('âœ“ ETF ä¸‹è½½å®Œæˆ!')
        logger.info(f'  æ€»æ•°: {stats["total"]}')
        logger.info(f'  å·²ä¸‹è½½: {stats["downloaded"]}')
        logger.info(f'  è·³è¿‡: {stats["skipped"]}')
        logger.info(f'  å¤±è´¥: {stats["failed"]}')
        logger.info(f'  æ–°å¢è®°å½•: {stats["records_added"]:,}')
        logger.info(f'  è€—æ—¶: {duration:.2f} ç§’ ({duration/60:.1f} åˆ†é’Ÿ)')
        logger.info(f'  å¹³å‡é€Ÿåº¦: {len(symbols)/duration:.2f} ä¸ª/ç§’')
        logger.info('')

        return stats

    def download_stock_history_optimized(self, start_date: str, end_date: str,
                                        force: bool = False) -> dict:
        """
        ä¼˜åŒ–çš„Aè‚¡å†å²æ•°æ®ä¸‹è½½ï¼ˆæ™ºèƒ½æ‰¹é‡æ¨¡å¼ï¼‰

        ä¸‰é˜¶æ®µç­–ç•¥:
        1. æ‰¹é‡æ£€æŸ¥å®Œæ•´æ€§ - å•æ¬¡æŸ¥è¯¢æ‰€æœ‰è‚¡ç¥¨
        2. åˆ†æ‰¹å¤„ç† - å°†éœ€è¦ä¸‹è½½çš„è‚¡ç¥¨åˆ†æˆæ‰¹æ¬¡
        3. å¹¶å‘ä¸‹è½½ + æ‰¹é‡æ’å…¥ - ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†æ‰¹æ¬¡

        Args:
            start_date: å¼€å§‹æ—¥æœŸ (YYYYMMDD)
            end_date: ç»“æŸæ—¥æœŸ (YYYYMMDD)
            force: æ˜¯å¦å¼ºåˆ¶é‡æ–°ä¸‹è½½

        Returns:
            dict: ç»Ÿè®¡ä¿¡æ¯
        """
        logger.info('='*60)
        logger.info('[é˜¶æ®µ2] ä¸‹è½½Aè‚¡å†å²æ•°æ® (æ™ºèƒ½æ‰¹é‡æ¨¡å¼)')
        logger.info('='*60)
        logger.info('')

        start = time.time()
        symbols = self.db.get_stock_codes()

        if not symbols:
            logger.warning('æ²¡æœ‰æ‰¾åˆ°è‚¡ç¥¨ä»£ç ,è·³è¿‡è‚¡ç¥¨ä¸‹è½½')
            return {'total': 0, 'downloaded': 0, 'skipped': 0, 'failed': 0, 'records_added': 0, 'duration': 0}

        # Phase 1: æ‰¹é‡æ£€æŸ¥å®Œæ•´æ€§
        logger.info(f'[Phase 1] æ£€æŸ¥ {len(symbols)} åªè‚¡ç¥¨çš„æ•°æ®å®Œæ•´æ€§...')
        completeness = self.db.get_stock_completeness_info(symbols, start_date)

        needs_download = [s for s, info in completeness.items()
                         if info['needs_download'] or force]
        already_complete = len(symbols) - len(needs_download)

        logger.info(f'  å·²å®Œæ•´: {already_complete} åª (è·³è¿‡)')
        logger.info(f'  éœ€ä¸‹è½½: {len(needs_download)} åª')
        logger.info('')

        if not needs_download:
            return {
                'total': len(symbols),
                'downloaded': 0,
                'skipped': len(symbols),
                'failed': 0,
                'records_added': 0,
                'duration': 0
            }

        # Phase 2: åˆ†æ‰¹å¤„ç†
        BATCH_SIZE = 50
        symbol_batches = [needs_download[i:i+BATCH_SIZE]
                         for i in range(0, len(needs_download), BATCH_SIZE)]

        logger.info(f'[Phase 2] å¤„ç† {len(symbol_batches)} ä¸ªæ‰¹æ¬¡ (æ¯æ‰¹{BATCH_SIZE}åª)...')
        logger.info(f'å¹¶å‘æ•°: {self.max_workers} ä¸ªçº¿ç¨‹')
        logger.info('')

        stats = {
            'total': len(symbols),
            'downloaded': 0,
            'skipped': already_complete,
            'failed': 0,
            'records_added': 0
        }

        # Phase 3: å¹¶å‘ä¸‹è½½æ‰¹æ¬¡
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {
                executor.submit(self._download_single_batch, batch, start_date, end_date): batch
                for batch in symbol_batches
            }

            with tqdm(total=len(symbol_batches), desc="è‚¡ç¥¨æ‰¹é‡ä¸‹è½½", unit="æ‰¹") as pbar:
                for future in as_completed(futures):
                    try:
                        batch_stats = future.result()
                        stats['downloaded'] += batch_stats['success']
                        stats['failed'] += batch_stats['failed']
                        stats['records_added'] += batch_stats['records_added']
                        self.failed_symbols['stock'].extend(batch_stats['failed_symbols'])

                        pbar.set_postfix({
                            'ä¸‹è½½': stats['downloaded'],
                            'å¤±è´¥': stats['failed']
                        })

                    except Exception as e:
                        logger.error(f'æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}')
                        stats['failed'] += len(futures[future])

                    pbar.update(1)

        duration = time.time() - start
        stats['duration'] = duration

        logger.info('')
        logger.info('âœ“ è‚¡ç¥¨ä¸‹è½½å®Œæˆ!')
        logger.info(f'  æ€»æ•°: {stats["total"]}')
        logger.info(f'  å·²ä¸‹è½½: {stats["downloaded"]}')
        logger.info(f'  è·³è¿‡: {stats["skipped"]}')
        logger.info(f'  å¤±è´¥: {stats["failed"]}')
        logger.info(f'  æ–°å¢è®°å½•: {stats["records_added"]:,}')
        logger.info(f'  è€—æ—¶: {duration:.2f} ç§’ ({duration/60:.1f} åˆ†é’Ÿ)')
        logger.info(f'  å¹³å‡é€Ÿåº¦: {len(symbols)/duration:.2f} åª/ç§’')
        logger.info('')

        return stats

    def _download_single_etf(self, symbol: str, start_date: str, end_date: str,
                             force: bool) -> dict:
        """
        ä¸‹è½½å•ä¸ªETFæ•°æ®ï¼ˆç”¨äºå¹¶å‘è°ƒç”¨ï¼‰

        Args:
            symbol: ETFä»£ç 
            start_date: ç›®æ ‡èµ·å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            force: æ˜¯å¦å¼ºåˆ¶é‡æ–°ä¸‹è½½

        Returns:
            dict: {'symbol': str, 'success': bool, 'skipped': bool, 'records': int, 'error': str}
        """
        try:
            # æ£€æŸ¥æœ€æ–°æ—¥æœŸ
            latest_date = self.db.get_latest_date(symbol)

            # å†³ç­–æ˜¯å¦éœ€è¦ä¸‹è½½
            decision = self._should_download(symbol, latest_date, start_date, force)

            if not decision.should_download:
                return {'symbol': symbol, 'success': False, 'skipped': True,
                       'records': 0, 'error': '', 'reason': decision.reason}

            # ä¸‹è½½æ•°æ®
            code = symbol.split('.')[0]
            df = self.etf_downloader.fetch_etf_history(code, decision.actual_start_date, end_date)

            if df is None or df.empty:
                return {'symbol': symbol, 'success': False, 'skipped': True,
                       'records': 0, 'error': 'æ— æ•°æ®', 'reason': 'æ— æ•°æ®'}

            # è½¬æ¢åˆ—å
            df.rename(columns={
                'æ—¥æœŸ': 'date', 'å¼€ç›˜': 'open', 'æ”¶ç›˜': 'close',
                'æœ€é«˜': 'high', 'æœ€ä½': 'low', 'æˆäº¤é‡': 'volume',
                'æˆäº¤é¢': 'amount', 'æŒ¯å¹…': 'amplitude', 'æ¶¨è·Œå¹…': 'change_pct',
                'æ¶¨è·Œé¢': 'change_amount', 'æ¢æ‰‹ç‡': 'turnover_rate'
            }, inplace=True)

            # å­˜å…¥æ•°æ®åº“
            success = self.db.append_etf_history(df, symbol)

            if success:
                return {'symbol': symbol, 'success': True, 'skipped': False,
                       'records': len(df), 'error': '', 'reason': decision.reason}
            else:
                return {'symbol': symbol, 'success': False, 'skipped': False,
                       'records': 0, 'error': 'æ•°æ®åº“æ’å…¥å¤±è´¥', 'reason': decision.reason}

        except Exception as e:
            return {'symbol': symbol, 'success': False, 'skipped': False,
                   'records': 0, 'error': str(e), 'reason': 'å¼‚å¸¸'}

    def download_etf_history(self, start_date: str, end_date: str,
                            force: bool = False) -> dict:
        """
        å¹¶å‘ä¸‹è½½ETFå†å²æ•°æ®

        Args:
            start_date: å¼€å§‹æ—¥æœŸ (YYYYMMDD)
            end_date: ç»“æŸæ—¥æœŸ (YYYYMMDD)
            force: æ˜¯å¦å¼ºåˆ¶é‡æ–°ä¸‹è½½

        Returns:
            dict: ç»Ÿè®¡ä¿¡æ¯
        """
        logger.info('='*60)
        logger.info('[é˜¶æ®µ1] ä¸‹è½½ETFå†å²æ•°æ® (å¹¶å‘æ¨¡å¼)')
        logger.info('='*60)
        logger.info('')

        start = time.time()
        symbols = self.db.get_etf_codes()

        if not symbols:
            logger.warning('æ²¡æœ‰æ‰¾åˆ° ETF ä»£ç ,è·³è¿‡ ETF ä¸‹è½½')
            return {'total': 0, 'downloaded': 0, 'skipped': 0, 'failed': 0, 'records_added': 0, 'duration': 0}

        stats = {
            'total': len(symbols),
            'downloaded': 0,
            'skipped': 0,
            'failed': 0,
            'records_added': 0
        }

        logger.info(f'å¾…æ£€æŸ¥: {len(symbols)} ä¸ª ETF')
        logger.info(f'å¹¶å‘æ•°: {self.max_workers} ä¸ªçº¿ç¨‹')
        logger.info('')

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘ä¸‹è½½
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            futures = {
                executor.submit(self._download_single_etf, symbol, start_date, end_date, force): symbol
                for symbol in symbols
            }

            # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
            with tqdm(total=len(symbols), desc="ETFä¸‹è½½", unit="ä¸ª") as pbar:
                for future in as_completed(futures):
                    symbol = futures[future]
                    try:
                        result = future.result()

                        if result['skipped']:
                            stats['skipped'] += 1
                        elif result['success']:
                            stats['downloaded'] += 1
                            stats['records_added'] += result['records']
                        else:
                            stats['failed'] += 1
                            self.failed_symbols['etf'].append(symbol)
                            if result.get('error'):
                                logger.debug(f'{symbol} å¤±è´¥: {result["error"]}')

                        pbar.set_postfix({
                            'ä¸‹è½½': stats['downloaded'],
                            'è·³è¿‡': stats['skipped'],
                            'å¤±è´¥': stats['failed']
                        })

                    except Exception as e:
                        logger.error(f'å¤„ç† {symbol} æ—¶å‘ç”Ÿå¼‚å¸¸: {e}')
                        stats['failed'] += 1
                        self.failed_symbols['etf'].append(symbol)

                    pbar.update(1)

        duration = time.time() - start
        stats['duration'] = duration

        logger.info('')
        logger.info('âœ“ ETF ä¸‹è½½å®Œæˆ!')
        logger.info(f'  æ€»æ•°: {stats["total"]}')
        logger.info(f'  å·²ä¸‹è½½: {stats["downloaded"]}')
        logger.info(f'  è·³è¿‡: {stats["skipped"]}')
        logger.info(f'  å¤±è´¥: {stats["failed"]}')
        logger.info(f'  æ–°å¢è®°å½•: {stats["records_added"]:,}')
        logger.info(f'  è€—æ—¶: {duration:.2f} ç§’ ({duration/60:.1f} åˆ†é’Ÿ)')
        logger.info(f'  å¹³å‡é€Ÿåº¦: {len(symbols)/duration:.2f} ä¸ª/ç§’')
        logger.info('')

        return stats

    def _download_single_stock(self, symbol: str, start_date: str, end_date: str,
                              force: bool) -> dict:
        """
        ä¸‹è½½å•ä¸ªè‚¡ç¥¨æ•°æ®ï¼ˆç”¨äºå¹¶å‘è°ƒç”¨ï¼‰

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            start_date: ç›®æ ‡èµ·å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            force: æ˜¯å¦å¼ºåˆ¶é‡æ–°ä¸‹è½½

        Returns:
            dict: {'symbol': str, 'success': bool, 'skipped': bool, 'records': int, 'error': str}
        """
        try:
            # æ£€æŸ¥æœ€æ–°æ—¥æœŸ
            latest_date = self.db.get_stock_latest_date(symbol)

            # å†³ç­–æ˜¯å¦éœ€è¦ä¸‹è½½
            decision = self._should_download(symbol, latest_date, start_date, force)

            if not decision.should_download:
                return {'symbol': symbol, 'success': False, 'skipped': True,
                       'records': 0, 'error': '', 'reason': decision.reason}

            # ä¸‹è½½æ•°æ®
            code = symbol.split('.')[0]
            df = self.stock_downloader.fetch_stock_history(code, decision.actual_start_date, end_date)

            if df is None or df.empty:
                return {'symbol': symbol, 'success': False, 'skipped': True,
                       'records': 0, 'error': 'æ— æ•°æ®', 'reason': 'æ— æ•°æ®'}

            # è½¬æ¢åˆ—å
            df.rename(columns={
                'æ—¥æœŸ': 'date', 'å¼€ç›˜': 'open', 'æ”¶ç›˜': 'close',
                'æœ€é«˜': 'high', 'æœ€ä½': 'low', 'æˆäº¤é‡': 'volume',
                'æˆäº¤é¢': 'amount', 'æŒ¯å¹…': 'amplitude', 'æ¶¨è·Œå¹…': 'change_pct',
                'æ¶¨è·Œé¢': 'change_amount', 'æ¢æ‰‹ç‡': 'turnover_rate'
            }, inplace=True)

            # å­˜å…¥æ•°æ®åº“
            success = self.db.append_stock_history(df, symbol)

            if success:
                return {'symbol': symbol, 'success': True, 'skipped': False,
                       'records': len(df), 'error': '', 'reason': decision.reason}
            else:
                return {'symbol': symbol, 'success': False, 'skipped': False,
                       'records': 0, 'error': 'æ•°æ®åº“æ’å…¥å¤±è´¥', 'reason': decision.reason}

        except Exception as e:
            return {'symbol': symbol, 'success': False, 'skipped': False,
                   'records': 0, 'error': str(e), 'reason': 'å¼‚å¸¸'}

    def download_stock_history(self, start_date: str, end_date: str,
                              force: bool = False) -> dict:
        """
        å¹¶å‘ä¸‹è½½Aè‚¡å†å²æ•°æ®

        Args:
            start_date: å¼€å§‹æ—¥æœŸ (YYYYMMDD)
            end_date: ç»“æŸæ—¥æœŸ (YYYYMMDD)
            force: æ˜¯å¦å¼ºåˆ¶é‡æ–°ä¸‹è½½

        Returns:
            dict: ç»Ÿè®¡ä¿¡æ¯
        """
        logger.info('='*60)
        logger.info('[é˜¶æ®µ2] ä¸‹è½½Aè‚¡å†å²æ•°æ® (å¹¶å‘æ¨¡å¼)')
        logger.info('='*60)
        logger.info('')

        start = time.time()
        symbols = self.db.get_stock_codes()

        if not symbols:
            logger.warning('æ²¡æœ‰æ‰¾åˆ°è‚¡ç¥¨ä»£ç ,è·³è¿‡è‚¡ç¥¨ä¸‹è½½')
            return {'total': 0, 'downloaded': 0, 'skipped': 0, 'failed': 0, 'records_added': 0, 'duration': 0}

        stats = {
            'total': len(symbols),
            'downloaded': 0,
            'skipped': 0,
            'failed': 0,
            'records_added': 0
        }

        logger.info(f'å¾…æ£€æŸ¥: {len(symbols)} åªè‚¡ç¥¨')
        logger.info(f'å¹¶å‘æ•°: {self.max_workers} ä¸ªçº¿ç¨‹')
        logger.info('')

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘ä¸‹è½½
        completed_count = 0
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            futures = {
                executor.submit(self._download_single_stock, symbol, start_date, end_date, force): symbol
                for symbol in symbols
            }

            # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
            with tqdm(total=len(symbols), desc="è‚¡ç¥¨ä¸‹è½½", unit="åª") as pbar:
                for future in as_completed(futures):
                    symbol = futures[future]
                    completed_count += 1
                    try:
                        result = future.result()

                        if result['skipped']:
                            stats['skipped'] += 1
                        elif result['success']:
                            stats['downloaded'] += 1
                            stats['records_added'] += result['records']
                        else:
                            stats['failed'] += 1
                            self.failed_symbols['stock'].append(symbol)
                            if result.get('error'):
                                logger.debug(f'{symbol} å¤±è´¥: {result["error"]}')

                        # æ¯100åªè‚¡ç¥¨æ›´æ–°ä¸€æ¬¡æ—¥å¿—
                        if completed_count % 100 == 0:
                            logger.info(f'è¿›åº¦: {completed_count}/{len(symbols)}')

                        pbar.set_postfix({
                            'ä¸‹è½½': stats['downloaded'],
                            'è·³è¿‡': stats['skipped'],
                            'å¤±è´¥': stats['failed']
                        })

                    except Exception as e:
                        logger.error(f'å¤„ç† {symbol} æ—¶å‘ç”Ÿå¼‚å¸¸: {e}')
                        stats['failed'] += 1
                        self.failed_symbols['stock'].append(symbol)

                    pbar.update(1)

        duration = time.time() - start
        stats['duration'] = duration

        logger.info('')
        logger.info('âœ“ è‚¡ç¥¨ä¸‹è½½å®Œæˆ!')
        logger.info(f'  æ€»æ•°: {stats["total"]}')
        logger.info(f'  å·²ä¸‹è½½: {stats["downloaded"]}')
        logger.info(f'  è·³è¿‡: {stats["skipped"]}')
        logger.info(f'  å¤±è´¥: {stats["failed"]}')
        logger.info(f'  æ–°å¢è®°å½•: {stats["records_added"]:,}')
        logger.info(f'  è€—æ—¶: {duration:.2f} ç§’ ({duration/60:.1f} åˆ†é’Ÿ)')
        logger.info(f'  å¹³å‡é€Ÿåº¦: {len(symbols)/duration:.2f} åª/ç§’')
        logger.info('')

        return stats

    def download_fundamental_snapshot(self) -> dict:
        """
        ä¸‹è½½åŸºæœ¬é¢æ•°æ®å¿«ç…§ï¼ˆä»…æœ€æ–°æ•°æ®ï¼Œä»…PEå’ŒPBï¼‰

        Returns:
            dict: ç»Ÿè®¡ä¿¡æ¯
        """
        logger.info('='*60)
        logger.info('[é˜¶æ®µ3] ä¸‹è½½åŸºæœ¬é¢æ•°æ®å¿«ç…§ (ä»…PE/PB)')
        logger.info('='*60)
        logger.info('')
        logger.info('æ³¨æ„: ä»…ä¸‹è½½æœ€æ–°åŸºæœ¬é¢å¿«ç…§ï¼Œä¸”åªåŒ…å«PE(å¸‚ç›ˆç‡)å’ŒPB(å¸‚å‡€ç‡)')
        logger.info('      å…¶ä»–è´¢åŠ¡æŒ‡æ ‡(ROE/ROA/å¸‚å€¼ç­‰)ä¸ä¸‹è½½ï¼Œæ•°æ®åº“ä¸­è®¾ä¸ºNULL')
        logger.info('      ä¼°å€¼å› å­(PE/PB)ä¸»è¦ç”¨äºæ¨ªæˆªé¢æ¯”è¾ƒï¼Œæœ€æ–°æ•°æ®å³å¯æ»¡è¶³éœ€æ±‚')
        logger.info('')

        start = time.time()

        # è·å–è‚¡ç¥¨ä»£ç åˆ—è¡¨
        symbols = self.db.get_stock_codes()

        if not symbols:
            logger.warning('æ²¡æœ‰æ‰¾åˆ°è‚¡ç¥¨ä»£ç ,è·³è¿‡åŸºæœ¬é¢ä¸‹è½½')
            return {'total': 0, 'success': 0, 'failed': 0, 'duration': 0}

        logger.info(f'å¾…æ›´æ–°: {len(symbols)} åªè‚¡ç¥¨')
        logger.info('')

        # ç›´æ¥è°ƒç”¨ç°æœ‰çš„åŸºæœ¬é¢ä¸‹è½½å™¨
        stats = self.fundamental_downloader.update_fundamental_data(symbols=symbols)

        duration = time.time() - start
        stats['duration'] = duration
        stats['total'] = len(symbols)

        logger.info('')
        logger.info('âœ“ åŸºæœ¬é¢ä¸‹è½½å®Œæˆ!')
        logger.info(f'  æ€»æ•°: {stats["total"]}')
        logger.info(f'  æˆåŠŸ: {stats["success"]}')
        logger.info(f'  å¤±è´¥: {stats["failed"]}')
        logger.info(f'  è€—æ—¶: {duration:.2f} ç§’ ({duration/60:.1f} åˆ†é’Ÿ)')
        logger.info('')

        return stats

    def download_historical_data(self, years: int = 5,
                                data_types: List[str] = None,
                                force: bool = False,
                                refresh_codes: bool = True) -> dict:
        """
        ä¸‹è½½å†å²æ•°æ®çš„ä¸»å…¥å£

        Args:
            years: å†å²å¹´æ•° (é»˜è®¤5å¹´)
            data_types: æ•°æ®ç±»å‹åˆ—è¡¨ï¼Œå¯é€‰: 'etf', 'stock', 'fundamental', 'all'
            force: æ˜¯å¦å¼ºåˆ¶é‡æ–°ä¸‹è½½å·²æœ‰æ•°æ®
            refresh_codes: æ˜¯å¦åˆ·æ–°ä»£ç è¡¨ (é»˜è®¤: True)

        Returns:
            dict: æ€»ä½“ç»Ÿè®¡ä¿¡æ¯
        """
        start_time = datetime.now()
        logger.info('')
        logger.info('*'*60)
        logger.info(f'å†å²æ•°æ®ä¸‹è½½æµç¨‹å¯åŠ¨ - {start_time.strftime("%Y-%m-%d %H:%M:%S")}')
        logger.info('*'*60)
        logger.info('')

        # é˜¶æ®µ0: æ£€æŸ¥ä»£ç è¡¨
        self.check_and_init_codes(force_refresh=refresh_codes)

        # è®¡ç®—æ—¥æœŸèŒƒå›´
        start_date, end_date = self.calculate_date_range(years)
        logger.info('')

        # è§„èŒƒåŒ–data_types
        if data_types is None or 'all' in data_types:
            data_types = ['etf', 'stock', 'fundamental']

        # æ‰§è¡Œå„é˜¶æ®µï¼ˆä½¿ç”¨ä¼˜åŒ–çš„æ‰¹é‡ä¸‹è½½æ–¹æ³•ï¼‰
        if 'etf' in data_types:
            stats = self.download_etf_history_optimized(start_date, end_date, force)
            self.stats.etf_stats = stats

        if 'stock' in data_types:
            time.sleep(2)  # ç¨ä½œåœé¡¿
            stats = self.download_stock_history_optimized(start_date, end_date, force)
            self.stats.stock_stats = stats

        if 'fundamental' in data_types:
            time.sleep(2)  # ç¨ä½œåœé¡¿
            stats = self.download_fundamental_snapshot()
            self.stats.fundamental_stats = stats

        # æ‰“å°æ€»ç»“
        self.stats.end_time = datetime.now()
        self._print_summary()
        self._save_failed_symbols()

        return {
            'etf': self.stats.etf_stats,
            'stock': self.stats.stock_stats,
            'fundamental': self.stats.fundamental_stats
        }

    def _print_summary(self):
        """æ‰“å°ä¸‹è½½æ€»ç»“"""
        total_duration = (self.stats.end_time - self.stats.start_time).total_seconds()

        logger.info('')
        logger.info('*'*60)
        logger.info('å†å²æ•°æ®ä¸‹è½½æ€»ç»“')
        logger.info('*'*60)

        # ETFç»Ÿè®¡
        if self.stats.etf_stats['total'] > 0:
            stats = self.stats.etf_stats
            logger.info('[ETFæ•°æ®]')
            logger.info(f'  æ€»æ•°: {stats["total"]}')
            logger.info(f'  å·²ä¸‹è½½: {stats["downloaded"]},  è·³è¿‡: {stats["skipped"]},  å¤±è´¥: {stats["failed"]}')
            logger.info(f'  æ–°å¢è®°å½•: {stats["records_added"]:,}')
            logger.info(f'  è€—æ—¶: {stats["duration"]:.2f} ç§’ ({stats["duration"]/60:.1f} åˆ†é’Ÿ)')
            logger.info('')

        # è‚¡ç¥¨ç»Ÿè®¡
        if self.stats.stock_stats['total'] > 0:
            stats = self.stats.stock_stats
            logger.info('[è‚¡ç¥¨æ•°æ®]')
            logger.info(f'  æ€»æ•°: {stats["total"]}')
            logger.info(f'  å·²ä¸‹è½½: {stats["downloaded"]},  è·³è¿‡: {stats["skipped"]},  å¤±è´¥: {stats["failed"]}')
            logger.info(f'  æ–°å¢è®°å½•: {stats["records_added"]:,}')
            logger.info(f'  è€—æ—¶: {stats["duration"]:.2f} ç§’ ({stats["duration"]/60:.1f} åˆ†é’Ÿ)')
            logger.info('')

        # åŸºæœ¬é¢ç»Ÿè®¡
        if self.stats.fundamental_stats['total'] > 0:
            stats = self.stats.fundamental_stats
            logger.info('[åŸºæœ¬é¢æ•°æ®]')
            logger.info(f'  æ€»æ•°: {stats["total"]}')
            logger.info(f'  æˆåŠŸ: {stats["success"]},  å¤±è´¥: {stats["failed"]}')
            logger.info(f'  è€—æ—¶: {stats["duration"]:.2f} ç§’ ({stats["duration"]/60:.1f} åˆ†é’Ÿ)')
            logger.info('')

        logger.info(f'æ€»è€—æ—¶: {total_duration:.2f} ç§’ ({total_duration/60:.1f} åˆ†é’Ÿ)')
        logger.info('*'*60)
        logger.info('')

    def _save_failed_symbols(self):
        """ä¿å­˜å¤±è´¥çš„ä»£ç åˆ—è¡¨"""
        if self.failed_symbols['etf'] or self.failed_symbols['stock']:
            failed_file = project_root / 'logs' / f'failed_symbols_{datetime.now().strftime("%Y%m%d_%H%M%S")}.txt'

            # ç¡®ä¿logsç›®å½•å­˜åœ¨
            failed_file.parent.mkdir(parents=True, exist_ok=True)

            with open(failed_file, 'w') as f:
                if self.failed_symbols['etf']:
                    f.write('# å¤±è´¥çš„ETF\n')
                    f.write('\n'.join(self.failed_symbols['etf']))
                    f.write('\n\n')

                if self.failed_symbols['stock']:
                    f.write('# å¤±è´¥çš„è‚¡ç¥¨\n')
                    f.write('\n'.join(self.failed_symbols['stock']))

            logger.info(f'å¤±è´¥ä»£ç å·²ä¿å­˜åˆ°: {failed_file}')
            logger.info('å¯ä»¥é‡æ–°è¿è¡Œè„šæœ¬ä»¥é‡è¯•å¤±è´¥çš„ä»£ç ')
            logger.info('')


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='ä¸‹è½½å†å²æ•°æ®åˆ°PostgreSQL (æ”¯æŒå¹¶å‘ä¸‹è½½)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  %(prog)s                                          # ä¸‹è½½é»˜è®¤5å¹´å†å²æ•°æ®(å…¨éƒ¨ç±»å‹)
  %(prog)s --years 3                               # ä¸‹è½½è¿‘3å¹´æ•°æ®
  %(prog)s --data-types etf stock                  # åªä¸‹è½½ETFå’Œè‚¡ç¥¨å†å²æ•°æ®
  %(prog)s --data-types fundamental                # åªä¸‹è½½åŸºæœ¬é¢å¿«ç…§
  %(prog)s --force                                 # å¼ºåˆ¶é‡æ–°ä¸‹è½½å·²æœ‰æ•°æ®
  %(prog)s --workers 10                            # ä½¿ç”¨10ä¸ªå¹¶å‘çº¿ç¨‹
  %(prog)s --years 10 --data-types all --force     # å¼ºåˆ¶é‡æ–°ä¸‹è½½è¿‘10å¹´æ‰€æœ‰æ•°æ®

æ³¨æ„:
  - ETFå’Œè‚¡ç¥¨æ•°æ®åˆ†åˆ«å­˜å…¥ etf_history å’Œ stock_history è¡¨
  - ä»£ç æ¸…å•åˆ†åˆ«å­˜å…¥ etf_codes å’Œ stock_codes è¡¨
  - åŸºæœ¬é¢æ•°æ®åªä¸‹è½½æœ€æ–°å¿«ç…§ï¼Œä¸”ä»…åŒ…å«PE(å¸‚ç›ˆç‡)å’ŒPB(å¸‚å‡€ç‡)
  - è„šæœ¬æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼Œé‡æ–°è¿è¡Œä¼šè‡ªåŠ¨è·³è¿‡å·²æœ‰æ•°æ®
  - ä½¿ç”¨ --force å¯å¼ºåˆ¶é‡æ–°ä¸‹è½½æ‰€æœ‰æ•°æ®
  - é»˜è®¤ä½¿ç”¨5ä¸ªå¹¶å‘çº¿ç¨‹ï¼Œå¯æ ¹æ®ç½‘ç»œæƒ…å†µè°ƒæ•´
        """
    )

    parser.add_argument(
        '--years', '-y',
        type=int,
        default=5,
        help='å†å²æ•°æ®å¹´æ•° (é»˜è®¤: 5)'
    )

    parser.add_argument(
        '--data-types', '-t',
        nargs='+',
        choices=['etf', 'stock', 'fundamental', 'all'],
        default=['all'],
        help='æ•°æ®ç±»å‹ (é»˜è®¤: all)'
    )

    parser.add_argument(
        '--force', '-f',
        action='store_true',
        help='å¼ºåˆ¶é‡æ–°ä¸‹è½½å·²æœ‰æ•°æ®'
    )

    parser.add_argument(
        '--workers', '-w',
        type=int,
        default=5,
        help='å¹¶å‘ä¸‹è½½æ•°é‡ (é»˜è®¤: 5, å»ºè®®3-10)'
    )

    parser.add_argument(
        '--refresh-codes',
        action='store_true',
        default=True,
        help='ä¸‹è½½å‰åˆ·æ–°ä»£ç è¡¨ (é»˜è®¤: True)'
    )

    parser.add_argument(
        '--no-refresh-codes',
        action='store_true',
        help='ä¸åˆ·æ–°ä»£ç è¡¨ï¼ˆä½¿ç”¨ç°æœ‰ä»£ç è¡¨ï¼‰'
    )

    args = parser.parse_args()

    # ç¡®å®šæ˜¯å¦åˆ·æ–°ä»£ç è¡¨
    refresh_codes = args.refresh_codes and not args.no_refresh_codes

    # åˆ›å»ºä¸‹è½½å™¨
    downloader = HistoricalDataDownloader(max_workers=args.workers)

    # æ‰§è¡Œä¸‹è½½
    try:
        downloader.download_historical_data(
            years=args.years,
            data_types=args.data_types,
            force=args.force,
            refresh_codes=refresh_codes
        )
    except KeyboardInterrupt:
        logger.warning('')
        logger.warning('ä¸‹è½½è¢«ç”¨æˆ·ä¸­æ–­')
        logger.warning('å¯ä»¥é‡æ–°è¿è¡Œè„šæœ¬ç»§ç»­ä¸‹è½½(ä¼šè‡ªåŠ¨è·³è¿‡å·²æœ‰æ•°æ®)')
        logger.warning('')
    except Exception as e:
        logger.error(f'ä¸‹è½½è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e}')
        raise


if __name__ == '__main__':
    main()
